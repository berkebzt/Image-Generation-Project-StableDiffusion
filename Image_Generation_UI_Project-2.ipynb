{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP5fXsaWu5gs"
      },
      "source": [
        "# DOCUMENTATION AND NOTES:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrW_QDUiNFn6"
      },
      "source": [
        "*** Everything used: ***\n",
        "\n",
        "Streamlit will be used to build the app: https://streamlit.io\n",
        "\n",
        "Diffusion model pretrained pipelines:\n",
        "* https://huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "* https://huggingface.co/stabilityai/stable-diffusion-2-1\n",
        "\n",
        "StudioGAN will be used for pretrained GAN models: https://github.com/POSTECH-CVLab/PyTorch-StudioGAN (Not used)\n",
        "\n",
        "One of the GAN models is FuseDream (CLIP + BigGAN):\n",
        " * article: https://analyticsindiamag.com/fusedream-a-hands-on-tutorial-on-this-text-to-image-generation-tool/\n",
        " * author repo: https://github.com/gnobitab/FuseDream\n",
        " * BigGAN repo: https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
        "\n",
        "Dreambooth:\n",
        "* Used BIRME for resizing images to 512x512: https://www.birme.net/?target_width=512&target_height=512\n",
        "\n",
        "* Used Dreambooth by Shivam Shrirao diffuser based repo: https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth (examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RohR3cjDVdZt"
      },
      "source": [
        "-------------------------\n",
        "\n",
        "Add features and qualities needed:\n",
        "\n",
        "\n",
        "8) Secondary page for showing the different model outputs. (REMOVED)\n",
        "\n",
        "   * (The outputs can be displayed in a carousel manner that shows which model is used on top left.) (use st.tabs while doing this)\n",
        "\n",
        "9) Red background with black and white text for general Akbank asthetic. (Sidebar title and headers and the general page title should be turned to white.)\n",
        "\n",
        "10) Area for viewing general model information and if desired, tweaking the parameters.)\n",
        "  \n",
        "   * (The areas are done however, general information regarding the models should be filled for all models.)\n",
        "   \n",
        "-------------------------\n",
        "\n",
        "* On the computation side, look up: 384x384 generation to superresolution (DLSS) and neural style transfer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLhIKgwvxCp9"
      },
      "source": [
        "Taskbar\n",
        "\n",
        "-------------------------\n",
        "POSSIBLE ADDITIONS FOR HIGHER QUALITY:\n",
        "\n",
        "1) Add scheduler selector:\n",
        "\n",
        "* Add K-LMS scheduler for SD1-4.\n",
        "\n",
        "* Add DPMSolverMultistep scheduler for SD2-1.\n",
        "\n",
        "2) Look up more parameters to further optimize stable diffusion.\n",
        "\n",
        "3) Add VQGAN + Clıp for variation:\n",
        "\n",
        "* Article: https://learn.adafruit.com/generating-ai-art-with-vqgan-clip/basic-use\n",
        "\n",
        "* Google Colab: https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT#scrollTo=VA1PHoJrRiK9\n",
        "\n",
        "4) Output three generated images (?)\n",
        "\n",
        "5) Instead of dreambooth try out LoRA for the fine-tuning for faster results and easier sharing, storage, and re-use. (LoRA is an improved finetuning method where instead of finetuning all the weights that constitute the weight matrix of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned.)\n",
        "\n",
        "-------------------------\n",
        "\n",
        "REMAINING:\n",
        "\n",
        "FlowBMs, implement their models, create and connect their parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9N0upAsikms"
      },
      "source": [
        "IMPORTANT:\n",
        "\n",
        "There is a CUDA memory problem because we are trying to import too many things.\n",
        "\n",
        "Placeholder solution: Import modules when you need, within the functions that are called based on model. (Implemented)\n",
        "\n",
        "Better solution: Import modules like above but unless the next model is the same type that requires the same modules delete the modules within the functions. (Need to keep track of the previous model type)  \n",
        "\n",
        "! Could not implement the above, deletion can only occur within functions when the function is called."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEw8fONYrsxR"
      },
      "source": [
        "GANS //\n",
        "\n",
        "\n",
        "Stuff to try out:\n",
        "\n",
        "* BigGAN: Is a large-scale GAN model developed by Google. It's designed for generating high-resolution images conditioned on text descriptions or class labels.\n",
        "\n",
        "* StyleGAN:  (Style Generative Adversarial Network) is a generative model architecture designed for generating high-resolution and highly realistic images, particularly faces and artworks.\n",
        "\n",
        " * StyleGAN2, which refines the architecture and training methods, and StyleGAN2-ADA (Adaptive Discriminator Augmentation), which further enhances training stability and image quality.\n",
        "\n",
        "* CLIP+GAN: Combining CLIP with GANs, you can condition the generation process on textual prompts and achieve text-to-image synthesis with improved context awareness.\n",
        "\n",
        "* AttnGAN: Attention Generative Adversarial Network (AttnGAN) is designed explicitly for fine-grained text-to-image generation. It uses attention mechanisms to focus on different parts of the text description while generating corresponding image details.\n",
        "\n",
        "* TAC-GAN: Text Conditioned Auxiliary Classifier Generative Adversarial Network, presents the study of generating realistic images through text-to-image synthesis with the help of GANs.\n",
        "\n",
        "-------------------------\n",
        "\n",
        "Possible GANs to implement: (Especially look at Stacked/Stack GANs if you have time.)\n",
        "\n",
        "* Stacked GAN: Stacked GAN refers to a general concept of stacking multiple GANs or GAN-like models on top of each other to improve the quality of generated images.\n",
        "\n",
        "  * StackGAN and StackGAN++: StackGAN is a particular implementation of stacked GANs designed for the task of text-to-image synthesis. It was developed to generate photo-realistic images from textual descriptions.\n",
        "\n",
        "  * Can use a StackGAN when you need to generate images from a completely different representation (e.g., from text-based descriptions).\n",
        "  *  authors repo: https://github.com/hanzhanggit/StackGAN\n",
        "\n",
        "* Information Maximizing GAN (Not verry useful for this specific task but can be fitted if needed.)\n",
        "\n",
        "* Super Resolution GAN:\n",
        "\n",
        "  *  Can use an SRGAN when you need to upscale images while recovering or preserving fine-grain, high-fidelity details. (Can be used to initially generate low dimensional images with other generational models, then be used to upscale them (?))\n",
        "\n",
        "-------------------------\n",
        "\n",
        "If everything else fails: pixelRNN, text-2-image, DiscoGAN, and IsGAN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbquYicGser"
      },
      "source": [
        "FLOW BASED GENERATION MODELS //\n",
        "\n",
        "* Glow\n",
        "* RealNVP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZlDXrkyMmJF"
      },
      "source": [
        "*** Dreambooth: Ideal parameters for cartoonish avatar generation: ***\n",
        "\n",
        "* for 8 training images\n",
        "\n",
        "  with_prior_preservation\n",
        "\n",
        "  train_batch_size=2 (effect not tested)\n",
        "\n",
        "  train_text_encoder\n",
        "\n",
        "  mixed_precision=\"fp16\"\n",
        "\n",
        "  use_8bit_adam\n",
        "\n",
        "  gradient_accumulation_steps=1\n",
        "\n",
        "  learning_rate=1e-6\n",
        "\n",
        "  num_class_images=100 (effect not tested)\n",
        "\n",
        "  sample_batch_size=4 (effect not tested)\n",
        "\n",
        "  max_train_steps=640 and 664\n",
        "  \n",
        "   * Previously tried 700 (and higher, untill 1200), presents a high risk of overfitting and every 1 image out of 5 is unusable. As of now the ideal number of training steps seems to be *** 80-83 epochs per image ***.\n",
        "\n",
        "  save_interval=10000\n",
        "\n",
        "  save_sample_prompt=\"yigitya person\"\n",
        "\n",
        "  ------------------------------------------------\n",
        "\n",
        "  prompt = \"A cartoonish avatar version of yigitya person.\"\n",
        "\n",
        "  negative_prompt = \"weird eyes, blurry eyes\"\n",
        "\n",
        "  num_samples = 1\n",
        "\n",
        "  guidance_scale = 9\n",
        "\n",
        "  num_inference_steps = 100\n",
        "\n",
        " ------------------------------------------------\n",
        "\n",
        " Accurate filenames were used, file names identical to instance prompt yigitya person. Also tried unrelated file names, ***does not seem to make a difference.***\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXu5fkUyvTML"
      },
      "source": [
        "# IMPLEMENTATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMiqDuguH_kR",
        "outputId": "5561426b-eb02-4a64-d561-55eed33652b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers\n",
            "  Downloading diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.4)\n",
            "Collecting huggingface-hub>=0.13.2 (from diffusers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Collecting safetensors>=0.3.1 (from diffusers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
            "Collecting huggingface-hub>=0.13.2 (from diffusers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, diffusers, transformers\n",
            "Successfully installed diffusers-0.21.4 huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.21.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.16.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n"
          ]
        }
      ],
      "source": [
        "# For application and stable diffusion models:\n",
        "!pip install streamlit -q\n",
        "!pip install --upgrade diffusers transformers scipy\n",
        "!pip install diffusers transformers accelerate scipy safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj7hAEVLB4zr"
      },
      "outputs": [],
      "source": [
        "# For GAN models:\n",
        "!git clone https://github.com/gnobitab/FuseDream.git\n",
        "!pip install ftfy regex tqdm numpy scipy h5py lpips==0.1.4\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install gdown\n",
        "!gdown 'https://drive.google.com/uc?id=1sOZ9og9kJLsqMNhaDnPJgzVsBZQ1sjZ5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PQxAx-n056w",
        "outputId": "af63a49c-c871-432b-8301-aabaeef8bb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biggan-512.pth\tFuseDream  sample_data\n",
            "/content/FuseDream\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "!cp biggan-512.pth FuseDream/BigGAN_utils/weights/\n",
        "%cd FuseDream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFu0lOjIGvI1",
        "outputId": "e8c3138d-fea4-4759-cf03-0a8b3c4f9841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.0.1+cu118 requires triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Dreambooth requirements:\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton                                                 # use either this or the one at the bottom\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naBgaWjoGxzP",
        "outputId": "3751dd48-445b-4f3e-9828-f62edf452565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ],
      "source": [
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moUs9zRIBa0z",
        "outputId": "e538d708-77df-4c61-e2f8-2c11515370e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting akbankavatargenerator.py\n"
          ]
        }
      ],
      "source": [
        "# V3: (Final Version)  -->  (Fix issue: Additional fine-tuning information within the app has red underline when deployed.)\n",
        "\n",
        "%%writefile akbankavatargenerator.py\n",
        "#####################################\n",
        "import streamlit as st\n",
        "import base64\n",
        "import torch\n",
        "from PIL import Image\n",
        "#####################################\n",
        "# Functions:\n",
        "\n",
        "# Function for calling any of the two stable diffusion models, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
        "def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "\n",
        "  if option == \"Stable Diffusion v1-4\":        # Uses a PNDM scheduler\n",
        "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "  elif option == \"Stable Diffusion 2-1\":\n",
        "    model_id = \"stabilityai/stable-diffusion-2-1\" # Uses a DDIM scheduler\n",
        "\n",
        "  device = \"cuda\"\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "  pipe = pipe.to(device)\n",
        "\n",
        "  prompt = user_prompt\n",
        "  negative_prompt = user_n_prompt\n",
        "\n",
        "  if situation:     # If custom parameters are chosen, parameters are given the user input values. If not, the pipeline is called without change.\n",
        "    num_inference_steps = i_inference_steps\n",
        "    guidance_scale = i_guidance_scale\n",
        "\n",
        "    if i_initial_seed is not None:        # In case of wanting to use custom seed, use the user input value. If not the pipeline is called without seed input.\n",
        "      generator = torch.Generator(\"cuda\").manual_seed(i_initial_seed)\n",
        "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]\n",
        "    else:\n",
        "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]\n",
        "  else:\n",
        "    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
        "\n",
        "  del StableDiffusionPipeline\n",
        "  return image\n",
        "\n",
        "# Function for calling FuseDream model, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
        "def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):\n",
        "  from tqdm import tqdm\n",
        "  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "  import torchvision\n",
        "  import BigGAN_utils.utils as utils\n",
        "  import clip\n",
        "  import torch.nn.functional as F\n",
        "  from DiffAugment_pytorch import DiffAugment\n",
        "  import numpy as np\n",
        "  from fusedream_utils import FuseDreamBaseGenerator, get_G\n",
        "  import sys\n",
        "\n",
        "  sentence = user_prompt\n",
        "\n",
        "  if use_seed:           # If the function is called using default parameter values SEED and use_seed are given the value None. If the utils.seed_rng(SEED) is called with None it gives an error, therefore the if statment is used.\n",
        "    utils.seed_rng(SEED)\n",
        "\n",
        "  sys.argv = ['']\n",
        "\n",
        "  G, config = get_G(512)\n",
        "  generator = FuseDreamBaseGenerator(G, config, 10)\n",
        "  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)\n",
        "\n",
        "  z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
        "  y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
        "  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)\n",
        "\n",
        "  image = img\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  image = pil_images[0]\n",
        "\n",
        "  del tqdm\n",
        "  del torchvision\n",
        "  del utils\n",
        "  del clip\n",
        "  del F\n",
        "  del DiffAugment\n",
        "  del FuseDreamBaseGenerator\n",
        "  del get_G\n",
        "  return image\n",
        "\n",
        "# Function for fine-tuning stable diffusion v1-5 model, imports and deletes required modules, parameters are self explanatory, returns the fine-tuned model pipeline:\n",
        "def finetunesdcall(u_token, u_class, plant, t_b_size, l_r, n_c_images, s_b_size, m_t_steps, uploaded_files):\n",
        "  # Settings (including model selection):\n",
        "\n",
        "  import subprocess\n",
        "\n",
        "  s_prompt=u_token+\" \"+u_class\n",
        "\n",
        "  MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "  OUTPUT_DIR = \"/content/stable_diffusion_weights/\" + u_token\n",
        "\n",
        "  try:\n",
        "      # Construct the shell command\n",
        "      command = f\"mkdir -p {OUTPUT_DIR}\"\n",
        "      # Execute the shell command\n",
        "      subprocess.run(command, shell=True, check=True)\n",
        "  except subprocess.CalledProcessError as e:\n",
        "      # Handle any errors that occur during command execution\n",
        "      print(f\"Error: {e}\")\n",
        "\n",
        "  # Choosing the instance and class prompts:\n",
        "\n",
        "  # You can also add multiple concepts here, try tweaking `--max_train_steps` accordingly.\n",
        "  # `class_data_dir` contains the regularization images\n",
        "  concepts_list = [\n",
        "      {\n",
        "          \"instance_prompt\":      f\"photo of {u_token} man\",\n",
        "          \"class_prompt\":         \"photo of a man\",\n",
        "          \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
        "          \"class_data_dir\":       \"/content/data/man\"\n",
        "      },\n",
        "      {\n",
        "          \"instance_prompt\":      f\"photo of {u_token} woman\",\n",
        "          \"class_prompt\":         \"photo of a woman\",\n",
        "          \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
        "          \"class_data_dir\":       \"/content/data/woman\"\n",
        "      },\n",
        "      {\n",
        "          \"instance_prompt\":      f\"photo of {u_token} person\",\n",
        "          \"class_prompt\":         \"photo of a person\",\n",
        "          \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
        "          \"class_data_dir\":       \"/content/data/person\"\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  import json\n",
        "  import os\n",
        "  for c in concepts_list:\n",
        "      os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "  keep_cl = []\n",
        "  for c in concepts_list:\n",
        "    a_class = c['instance_prompt'].split()\n",
        "    if a_class[-1] == u_class:\n",
        "      keep_cl.append(c)\n",
        "\n",
        "  with open(\"concepts_list.json\", \"w\") as f:\n",
        "      json.dump(keep_cl, f, indent=4)\n",
        "\n",
        "  # Upload your images\n",
        "\n",
        "  from google.colab import files\n",
        "  import shutil\n",
        "\n",
        "  for c in concepts_list:\n",
        "    a_class = c['instance_prompt'].split()\n",
        "    if a_class[-1] == u_class:\n",
        "      uploaded = uploaded_files\n",
        "      for element in uploaded:\n",
        "        filename = element.name\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        with open(dst_path, \"wb\") as f:\n",
        "            f.write(element.read())\n",
        "\n",
        "  # Fine-tuning the SD model:\n",
        "\n",
        "  # Tweak the parameters for desired image quality --> 1200 training steps = 40 newly introdued images for 30 epochs.\n",
        "  command = f\"\"\"\n",
        "          python3 train_dreambooth.py \\\n",
        "          --pretrained_model_name_or_path={MODEL_NAME} \\\n",
        "          --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "          --output_dir={OUTPUT_DIR} \\\n",
        "          --revision=\"fp16\" \\\n",
        "          --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "          --seed={plant} \\\n",
        "          --resolution=512 \\\n",
        "          --train_batch_size={t_b_size} \\\n",
        "          --train_text_encoder \\\n",
        "          --mixed_precision=\"fp16\" \\\n",
        "          --use_8bit_adam \\\n",
        "          --gradient_accumulation_steps=1 \\\n",
        "          --learning_rate={l_r} \\\n",
        "          --lr_scheduler=\"constant\" \\\n",
        "          --lr_warmup_steps=0 \\\n",
        "          --num_class_images={n_c_images} \\\n",
        "          --sample_batch_size={s_b_size} \\\n",
        "          --max_train_steps={m_t_steps} \\\n",
        "          --save_interval=10000 \\\n",
        "          --save_sample_prompt=\"{s_prompt}\" \\\n",
        "          --concepts_list=\"concepts_list.json\"\n",
        "      \"\"\"\n",
        "  try:\n",
        "      # Execute the shell command\n",
        "      subprocess.run(command, shell=True, check=True, executable=\"/bin/bash\")\n",
        "  except subprocess.CalledProcessError as e:\n",
        "      # Handle any errors that occur during command execution\n",
        "      print(f\"Error: {e}\")\n",
        "\n",
        "  # Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "  # `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples)\n",
        "\n",
        "  # Specify the weights directory to use (leave blank for latest):\n",
        "\n",
        "  from natsort import natsorted\n",
        "  from glob import glob\n",
        "  WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "\n",
        "  # Inference:\n",
        "\n",
        "  import torch\n",
        "  from torch import autocast\n",
        "  from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "  from IPython.display import display\n",
        "\n",
        "  model_path = WEIGHTS_DIR\n",
        "\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "  pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "  pipe.enable_xformers_memory_efficient_attention()\n",
        "  g_cuda = None\n",
        "\n",
        "  del json\n",
        "  del os\n",
        "  del files\n",
        "  del shutil\n",
        "  del natsorted\n",
        "  del glob\n",
        "  del torch\n",
        "  del autocast\n",
        "  del display\n",
        "  del subprocess\n",
        "  return pipe\n",
        "\n",
        "# Function for generating prompt conditioned images with fine-tuned stable diffusion v1-5 model, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
        "def dbgeneratecall(pipe, user_prompt, user_n_prompt, guidance_scale, num_inference_steps, use_seed, s33d):\n",
        "\n",
        "  import torch\n",
        "  from torch import autocast\n",
        "  from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "\n",
        "  # Run for generating images.\n",
        "\n",
        "  prompt = user_prompt\n",
        "  negative_prompt = user_n_prompt\n",
        "  num_samples = 1\n",
        "  height = 512\n",
        "  width = 512\n",
        "\n",
        "  if use_seed:\n",
        "    g_cuda = torch.Generator(device='cuda')\n",
        "    seed = s33d\n",
        "    g_cuda.manual_seed(seed)\n",
        "    with autocast(\"cuda\"), torch.inference_mode():\n",
        "        images = pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=num_samples,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=g_cuda\n",
        "        ).images\n",
        "  else:\n",
        "    with autocast(\"cuda\"), torch.inference_mode():\n",
        "        images = pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=num_samples,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "        ).images\n",
        "\n",
        "  for img in images:\n",
        "    generated = img\n",
        "\n",
        "  del torch\n",
        "  del autocast\n",
        "  return generated\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "def main():\n",
        "\n",
        "  icon = Image.open('/content/icon.png')\n",
        "  st.set_page_config(page_icon = icon, layout=\"wide\")\n",
        "\n",
        "  #background_image = Image.open('/content/background.jpg')\n",
        "\n",
        "  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))\n",
        "\n",
        "  image = Image.open('/content/akbank.jpg')\n",
        "  image2 = Image.open('/content/sabancı.png')\n",
        "\n",
        "  if 'files_uploaded' not in st.session_state:  # Keeps track of if any image has been uploaded (keeps boolean value). Initializing it beforehand, prevents being called before being initialized error\n",
        "    st.session_state.files_uploaded = False\n",
        "\n",
        "  if 'pictures' not in st.session_state:  # Keeps track of if any image has been uploaded (keeps the pictures themselves). Initializing it beforehand, prevents being called before being initialized error\n",
        "    st.session_state.pictures = False\n",
        "\n",
        "  if 'pipe_s' not in st.session_state:  # Keeps track of if pipe_s has been returned by finetunesdcall (keeps the pipeline). Initializing it beforehand, prevents being called before being initialized error\n",
        "    st.session_state.pipe_s = False\n",
        "\n",
        "  if 'fine_tuned' not in st.session_state:  # Keeps track of if the model has been fine-tuned. Initializing it beforehand, prevents being called before being initialized error\n",
        "    st.session_state.fine_tuned = False\n",
        "\n",
        "  with col1:\n",
        "    st.image(image, caption='', width = 500, use_column_width=True)\n",
        "    with st.sidebar:\n",
        "      st.title(':red[MODELS]')\n",
        "      st.header('General model information', divider='red')\n",
        "\n",
        "      with st.expander(\"Stable Diffusion v1-4\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"Stable Diffusion 2-1\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"FuseDream (CLIP+BigGAN)\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"Non-specified Flow based model\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      st.header('Settings to tweak the models', divider='red')\n",
        "      # situation = True --> Want to use custom parameters, situation = False --> Want to use default parameters\n",
        "      situation = st.toggle('Use custom parameters for models')\n",
        "      if situation:\n",
        "        with st.expander('Diffusion model parameters:'):  # Have 3 parameters\n",
        "\n",
        "          i_inference_steps = st.number_input('Number of inference steps', step = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)\n",
        "          else:\n",
        "            i_initial_seed = None\n",
        "\n",
        "          i_guidance_scale = st.slider('Guidance scale', 1.0, 25.0, 7.5, step = 0.5)\n",
        "\n",
        "        with st.expander('FuseDream (CLIP+BigGAN) parameters:'): # Has 5 parameters (including the boolean use_seed)\n",
        "\n",
        "          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)\n",
        "\n",
        "          OPT_ITERS = st.number_input('Number of iterations', step = 1)\n",
        "\n",
        "          NUM_BASIS = st.number_input('Number of basis images', step = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
        "          else:\n",
        "            SEED = None\n",
        "\n",
        "        with st.expander('Non-specified Flow based model parameters:'): # Not yet implemented, has 0 parameters\n",
        "          st.write(\"PLaceholder.\")\n",
        "\n",
        "      st.header(' ', divider='red')\n",
        "      # Dreambooth is separated from the situation block since it should not be called with default parameters as the parameters need to be entered according to the external data and its use. (TLDR Attune parameters per each use for good results.)\n",
        "      with st.expander('Dreambooth parameters:'): # Has 8 parameters for fine-tuning and 4 parameters for image generation (including use_seed boolean)\n",
        "\n",
        "        u_token = st.text_input('Unique token', '', placeholder='Enter a unique token.', help = 'example: name_surname or zwx')\n",
        "\n",
        "        u_class = st.text_input('Class of reg images', '', placeholder='Enter a regularization image set class.', help = 'Available class names: man, woman, person')\n",
        "\n",
        "        plant = st.number_input('Initial seed', value = 0, step = 1, key = 3)\n",
        "\n",
        "        t_b_size = st.number_input('Training bach size', step =1, key = 1)\n",
        "\n",
        "        l_r = st.slider('Learning rate', 1, 6, step = 1)\n",
        "        l_r = l_r * 1e-6\n",
        "\n",
        "        n_c_images = st.number_input('Number of class images', step =1)\n",
        "\n",
        "        s_b_size = st.number_input('Sample batch size', step =1)\n",
        "\n",
        "        m_t_steps = st.number_input('Training steps', step =1)\n",
        "\n",
        "        ft_start = st.button('Start fine-tuning')\n",
        "\n",
        "        if ft_start and st.session_state.files_uploaded:   # Checks if any photos have been uploaded before starting fine-tuning, if not gives an error\n",
        "          process = st.empty()            # Creating a Fine-tuning...\n",
        "          process.text('Fine-tuning...')\n",
        "\n",
        "          st.session_state.pipe_s = finetunesdcall(u_token, u_class, plant, t_b_size, l_r, n_c_images, s_b_size, m_t_steps, st.session_state.pictures)              # Calls dreambooth fine-tuning function\n",
        "          st.session_state.fine_tuned = True\n",
        "\n",
        "          process.empty()        # Deletes Fine-tuning... text\n",
        "          st.write('Fine-tuning completed, now an image can be generated')\n",
        "        elif ft_start and not st.session_state.files_uploaded:\n",
        "          st.error('Please upload images to fine-tune the model', icon=\"🚨\")\n",
        "\n",
        "        guidance_scale = st.slider('Guidance scale', 1.0, 25.0, 7.5, step = 0.5, key = 12)\n",
        "\n",
        "        num_inference_steps = st.number_input('Number of inference steps', step = 1, key = 13)\n",
        "\n",
        "        use_seed = st.toggle('Enter a seed value', key = 14, help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "        if use_seed:\n",
        "          s33d = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
        "        else:\n",
        "          s33d = None\n",
        "\n",
        "  with col2:\n",
        "    st.title(\"Akbank Avatar Generator\")\n",
        "    use_db = st.toggle('Use Dreambooth')  # If you want to use Dreambooth, reveals the following untill the line:\n",
        "    if use_db:\n",
        "      st.write('Show us what you look like 😊')\n",
        "      uploaded_files = st.file_uploader(\"Upload your photos here...\", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')\n",
        "      if uploaded_files:\n",
        "        st.session_state.pictures = uploaded_files\n",
        "        st.session_state.files_uploaded = True\n",
        "      information = st.checkbox('See additional information for fine tuned image generation with Dreambooth')\n",
        "      if information:               # Additional infromation toggle\n",
        "        display_text = \"\"\"\n",
        "        Ideal parameters for cartoonish avatar generation:\n",
        "\n",
        "        (Used 8 inputted training images)\n",
        "        ▪️ Training batch size = 2 (effect not tested)\n",
        "        ▪️ Learning rate = 1e-6\n",
        "        ▪️ Number of class images = 100 (effect not tested)\n",
        "        ▪️ Sample batch size = 4 (effect not tested)\n",
        "        ▪️ Training steps = 640 and 664\n",
        "          (The ideal number of training steps seems to be 80-83 epochs per image.)\n",
        "        ▪️ Sample prompt = unique token + class of regularization images\n",
        "        ▪️ Prompt = A cartoonish avatar version of Sample prompt.\n",
        "        ▪️ Negative prompt = Up to the user\n",
        "        ▪️ Guidance scale = 9\n",
        "        ▪️ Number of inference steps = 100\n",
        "        \"\"\"\n",
        "        st.text_area(\"Ideal parameters for different prompts and uses:\", display_text)\n",
        "\n",
        "      st.write(':red[Attention:] Dreambooth is currently only available for Stable-Diffusion-v1-5. When using Dreambooth this model is automatically implemented, no need to choose models 😅')\n",
        "################################################################################\n",
        "    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = \"Select your model...\")\n",
        "\n",
        "    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')\n",
        "\n",
        "    user_n_prompt = st.text_input('Negative prompt (Optional)', '')\n",
        "    st.write(':red[Attention:] Negative prompts are currently only available for diffusion models')\n",
        "\n",
        "    generation_start = st.button('Start generating')\n",
        "    if generation_start and user_prompt == '':          # Ensures a prompt is entered\n",
        "      st.error('Please enter a prompt', icon=\"🚨\")\n",
        "    elif generation_start and user_prompt != '':\n",
        "\n",
        "      select_sd = option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1'\n",
        "\n",
        "      process = st.empty()            # Creating a Generating...\n",
        "      process.text('Generating...')\n",
        "\n",
        "      if use_db:        # Selected model is Dreambooth fine tuning on Stable Diffusion v1-5\n",
        "        if st.session_state.fine_tuned:\n",
        "          generated_image = dbgeneratecall(st.session_state.pipe_s, user_prompt, user_n_prompt, guidance_scale, num_inference_steps, use_seed, s33d)\n",
        "\n",
        "          process.empty()            # Deletes Generating... text\n",
        "          st.write('')\n",
        "\n",
        "          st.image(generated_image, caption= 'Generated image using Dreambooth fine-tuning.')       # Display the generated image with caption\n",
        "        else:\n",
        "          process.empty()            # Deletes Generating... text\n",
        "          st.error('Please first fine-tune the model', icon=\"🚨\")\n",
        "\n",
        "      elif select_sd:       # Selected model is a Stable Diffusion one\n",
        "        if not situation:\n",
        "          i_inference_steps = 0         # If chosen to use default parameters, gives placeholder values that will be changed to default values within the stablediffusioncall function\n",
        "          i_guidance_scale = 0\n",
        "          i_initial_seed = 0\n",
        "\n",
        "        generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)       # Calls stablediffusioncall function\n",
        "\n",
        "        process.empty()            # Deletes Generating... text\n",
        "        st.write('')\n",
        "\n",
        "        st.image(generated_image, caption= f'Generated image using {option}.')       # Display the generated image with caption\n",
        "\n",
        "      elif option == 'FuseDream (CLIP+BigGAN)':    # Selected model is FuseDream\n",
        "        if not situation:\n",
        "          INIT_ITERS =  1000\n",
        "          OPT_ITERS = 1000                 # If chosen to use default parameters, gives default values\n",
        "          NUM_BASIS = 10\n",
        "          SEED = None\n",
        "          use_seed = False\n",
        "\n",
        "        generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)            # Calls fusedreamcall function\n",
        "\n",
        "        process.empty()           # Deletes Generating... text\n",
        "        st.write('')\n",
        "\n",
        "        st.image(generated_image, caption= f'Generated image using {option}.')    # Display the generated image with caption\n",
        "\n",
        "      elif option == 'Non-specified Flow based model':      # Selected model is a Non-specified Flow based model\n",
        "        process.empty()                  # Deletes Generating... text\n",
        "        st.write('')\n",
        "\n",
        "        st.write('Error: This model is not implemented yet', key = 51)\n",
        "\n",
        "  with col3:\n",
        "    st.image(image2, caption='', width = 150, use_column_width=True, output_format=\"PNG\")\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLMwq1e-KcYY",
        "outputId": "8ef02bc7-9995-4a37-9db2-ba1586d3c580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.248.186"
          ]
        }
      ],
      "source": [
        "!curl ipecho.net/plain   # -----> Enter with remote IP address"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYr8hRh4JCLT",
        "outputId": "b34423af-6007-4fd8-b162-61693aeb00cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.248.186:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.5s\n",
            "your url is: https://wet-icons-listen.loca.lt\n",
            "2023-10-04 17:35:56.352985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading (…)lve/main/config.json: 100% 547/547 [00:00<00:00, 3.15MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 335M/335M [00:02<00:00, 132MB/s] \n",
            "Downloading (…)p16/model_index.json: 100% 543/543 [00:00<00:00, 3.06MB/s]\n",
            "text_encoder/model.safetensors not found\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n",
            "Downloading (…)_checker/config.json: 100% 4.70k/4.70k [00:00<00:00, 18.7MB/s]\n",
            "\n",
            "Downloading (…)tokenizer/merges.txt:   0% 0.00/525k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading (…)_encoder/config.json: 100% 636/636 [00:00<00:00, 3.62MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 3.19MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)cheduler_config.json: 100% 307/307 [00:00<00:00, 2.08MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)rocessor_config.json: 100% 342/342 [00:00<00:00, 2.31MB/s]\n",
            "Fetching 15 files:   7% 1/15 [00:01<00:17,  1.23s/it]\n",
            "\n",
            "Downloading pytorch_model.bin:   0% 0.00/608M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 3.78MB/s]\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 822/822 [00:00<00:00, 3.32MB/s]\n",
            "\n",
            "Downloading (…)tokenizer/vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)cc2/unet/config.json: 100% 806/806 [00:00<00:00, 4.96MB/s]\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:   2% 10.5M/608M [00:00<00:07, 79.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:   0% 0.00/246M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)9cc2/vae/config.json: 100% 609/609 [00:00<00:00, 64.8kB/s]\n",
            "\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 5.86MB/s]\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:   5% 31.5M/608M [00:00<00:05, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:   4% 10.5M/246M [00:00<00:03, 68.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:   9% 52.4M/608M [00:00<00:05, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  13% 31.5M/246M [00:00<00:02, 86.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  12% 73.4M/608M [00:00<00:04, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  21% 52.4M/246M [00:00<00:01, 102MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  16% 94.4M/608M [00:00<00:03, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   0% 0.00/1.72G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:   6% 10.5M/167M [00:00<00:04, 36.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  30% 73.4M/246M [00:00<00:01, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   1% 10.5M/1.72G [00:00<00:22, 77.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  13% 21.0M/167M [00:00<00:03, 47.7MB/s]\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  19% 115M/608M [00:01<00:04, 109MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  34% 83.9M/246M [00:00<00:01, 91.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   1% 21.0M/1.72G [00:00<00:20, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  19% 31.5M/167M [00:00<00:02, 60.9MB/s]\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  22% 136M/608M [00:01<00:04, 105MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  25% 41.9M/167M [00:00<00:01, 68.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   2% 41.9M/1.72G [00:00<00:16, 99.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  43% 105M/246M [00:01<00:01, 94.4MB/s] \u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  31% 52.4M/167M [00:00<00:01, 75.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  47% 115M/246M [00:01<00:01, 92.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   3% 52.4M/1.72G [00:00<00:18, 89.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  26% 157M/608M [00:01<00:04, 100MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  38% 62.9M/167M [00:00<00:01, 75.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   4% 62.9M/1.72G [00:00<00:18, 88.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  51% 126M/246M [00:01<00:01, 90.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  28% 168M/608M [00:01<00:04, 96.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   4% 73.4M/1.72G [00:00<00:19, 86.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  44% 73.4M/167M [00:01<00:01, 71.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  55% 136M/246M [00:01<00:01, 86.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  29% 178M/608M [00:01<00:04, 86.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  50% 83.9M/167M [00:01<00:01, 79.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  60% 147M/246M [00:01<00:01, 86.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   5% 83.9M/1.72G [00:01<00:20, 79.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  31% 189M/608M [00:01<00:05, 83.4MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  56% 94.4M/167M [00:01<00:00, 77.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   5% 94.4M/1.72G [00:01<00:19, 83.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  64% 157M/246M [00:01<00:01, 80.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   6% 105M/1.72G [00:01<00:22, 71.6MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  63% 105M/167M [00:01<00:00, 64.4MB/s] \u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  68% 168M/246M [00:01<00:01, 67.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  34% 210M/608M [00:02<00:04, 82.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   7% 115M/1.72G [00:01<00:21, 73.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  69% 115M/167M [00:01<00:00, 67.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  72% 178M/246M [00:02<00:00, 69.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  36% 220M/608M [00:02<00:05, 77.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  75% 126M/167M [00:01<00:00, 66.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  77% 189M/246M [00:02<00:00, 66.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   8% 136M/1.72G [00:01<00:22, 71.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  81% 199M/246M [00:02<00:00, 71.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  38% 231M/608M [00:02<00:06, 62.7MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  81% 136M/167M [00:02<00:00, 66.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  85% 210M/246M [00:02<00:00, 76.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  40% 241M/608M [00:02<00:05, 62.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:   9% 157M/1.72G [00:02<00:21, 73.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  88% 147M/167M [00:02<00:00, 57.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  89% 220M/246M [00:02<00:00, 73.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin:  94% 157M/167M [00:02<00:00, 63.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  10% 168M/1.72G [00:02<00:21, 73.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  41% 252M/608M [00:02<00:06, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  94% 231M/246M [00:02<00:00, 72.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)on_pytorch_model.bin: 100% 167M/167M [00:02<00:00, 67.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin: 100% 167M/167M [00:02<00:00, 63.5MB/s]\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  43% 262M/608M [00:03<00:06, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading pytorch_model.bin:  98% 241M/246M [00:03<00:00, 65.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  11% 189M/1.72G [00:02<00:20, 75.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 246M/246M [00:03<00:00, 77.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  12% 199M/1.72G [00:02<00:20, 73.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  48% 294M/608M [00:03<00:04, 78.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  13% 220M/1.72G [00:02<00:17, 86.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  52% 315M/608M [00:03<00:03, 90.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  13% 231M/1.72G [00:02<00:17, 87.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  55% 336M/608M [00:03<00:02, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  15% 252M/1.72G [00:03<00:16, 87.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  15% 262M/1.72G [00:03<00:17, 85.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  59% 357M/608M [00:04<00:02, 95.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  16% 273M/1.72G [00:03<00:18, 78.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  60% 367M/608M [00:04<00:02, 86.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  16% 283M/1.72G [00:03<00:18, 79.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  62% 377M/608M [00:04<00:02, 85.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  66% 398M/608M [00:04<00:02, 97.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  18% 304M/1.72G [00:03<00:16, 87.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  18% 315M/1.72G [00:03<00:15, 88.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  69% 419M/608M [00:04<00:01, 106MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  19% 325M/1.72G [00:03<00:15, 92.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  72% 440M/608M [00:04<00:01, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  20% 346M/1.72G [00:04<00:12, 111MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  78% 472M/608M [00:04<00:00, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  21% 367M/1.72G [00:04<00:10, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  83% 503M/608M [00:05<00:00, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  23% 388M/1.72G [00:04<00:09, 138MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  86% 524M/608M [00:05<00:00, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  24% 409M/1.72G [00:04<00:08, 149MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  90% 545M/608M [00:05<00:00, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  25% 430M/1.72G [00:04<00:09, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  93% 566M/608M [00:05<00:00, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  26% 451M/1.72G [00:04<00:09, 130MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin:  97% 587M/608M [00:05<00:00, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  27% 472M/1.72G [00:05<00:09, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 608M/608M [00:05<00:00, 103MB/s]\n",
            "Fetching 15 files:  27% 4/15 [00:07<00:20,  1.83s/it]\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  29% 493M/1.72G [00:05<00:09, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  30% 524M/1.72G [00:05<00:07, 161MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  32% 556M/1.72G [00:05<00:06, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  34% 587M/1.72G [00:05<00:05, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  36% 619M/1.72G [00:05<00:05, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  38% 650M/1.72G [00:05<00:04, 216MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  40% 682M/1.72G [00:05<00:04, 221MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  41% 713M/1.72G [00:06<00:04, 221MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  43% 744M/1.72G [00:06<00:04, 227MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  45% 776M/1.72G [00:06<00:04, 229MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  47% 807M/1.72G [00:06<00:03, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  49% 839M/1.72G [00:06<00:03, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  51% 870M/1.72G [00:06<00:03, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  52% 902M/1.72G [00:06<00:03, 225MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  54% 933M/1.72G [00:07<00:03, 227MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  56% 965M/1.72G [00:07<00:03, 226MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  58% 996M/1.72G [00:07<00:03, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  60% 1.03G/1.72G [00:07<00:02, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  62% 1.06G/1.72G [00:07<00:02, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  63% 1.09G/1.72G [00:07<00:02, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  65% 1.12G/1.72G [00:07<00:02, 225MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  67% 1.15G/1.72G [00:07<00:02, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  69% 1.18G/1.72G [00:08<00:02, 242MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  71% 1.22G/1.72G [00:08<00:02, 250MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  73% 1.25G/1.72G [00:08<00:01, 257MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  74% 1.28G/1.72G [00:08<00:01, 261MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  76% 1.31G/1.72G [00:08<00:01, 258MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  78% 1.34G/1.72G [00:08<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  80% 1.37G/1.72G [00:08<00:01, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  82% 1.41G/1.72G [00:08<00:01, 238MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  84% 1.44G/1.72G [00:09<00:01, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  85% 1.47G/1.72G [00:09<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  87% 1.50G/1.72G [00:09<00:00, 242MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  89% 1.53G/1.72G [00:09<00:00, 243MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  91% 1.56G/1.72G [00:09<00:00, 244MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  93% 1.59G/1.72G [00:09<00:00, 245MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  95% 1.63G/1.72G [00:09<00:00, 242MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  96% 1.66G/1.72G [00:10<00:00, 243MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin:  98% 1.69G/1.72G [00:10<00:00, 237MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)on_pytorch_model.bin: 100% 1.72G/1.72G [00:10<00:00, 166MB/s]\n",
            "Fetching 15 files: 100% 15/15 [00:12<00:00,  1.21it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
            "  warnings.warn(\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "10/04/2023 17:36:26 - INFO - __main__ - Number of class images to sample: 100.\n",
            "Generating class images: 100% 25/25 [05:16<00:00, 12.66s/it]\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-6elak6xmo4bf --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:203: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "Downloading (…)cheduler_config.json: 100% 308/308 [00:00<00:00, 1.46MB/s]\n",
            "Caching latents: 100% 50/50 [00:19<00:00,  2.55it/s]\n",
            "10/04/2023 17:42:19 - INFO - __main__ - ***** Running training *****\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Num examples = 50\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Num batches each epoch = 50\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Num Epochs = 13\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Instantaneous batch size per device = 2\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "10/04/2023 17:42:19 - INFO - __main__ -   Total optimization steps = 640\n",
            "Steps: 100% 640/640 [12:13<00:00,  1.13s/it, loss=0.251, lr=1e-6]text_encoder/model.safetensors not found\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:06<00:19,  6.44s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:10<00:09,  4.75s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:13<00:04,  4.24s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:17<00:00,  4.32s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/yigitya/640\n",
            "Steps: 100% 640/640 [13:12<00:00,  1.24s/it, loss=0.251, lr=1e-6]\n",
            "2023-10-04 17:55:40.509401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "100% 100/100 [00:17<00:00,  5.63it/s]\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run akbankavatargenerator.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cPOrXbFkvsd"
      },
      "source": [
        "OLD VERSIONS:\n",
        "-------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxMpchvrIhlT"
      },
      "outputs": [],
      "source": [
        "# V1:\n",
        "\n",
        "%%writefile akbankavatargenerator.py\n",
        "#####################################\n",
        "import streamlit as st\n",
        "import base64\n",
        "import torch\n",
        "from PIL import Image\n",
        "#####################################\n",
        "# Functions:\n",
        "\n",
        "def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "\n",
        "  if option == \"Stable Diffusion v1-4\":        # Uses PNDM scheduler\n",
        "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "  elif option == \"Stable Diffusion 2-1\":\n",
        "    model_id = \"stabilityai/stable-diffusion-2-1\" # Uses DDIM scheduler\n",
        "\n",
        "  device = \"cuda\"\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "  pipe = pipe.to(device)\n",
        "\n",
        "  prompt = user_prompt\n",
        "  negative_prompt = user_n_prompt\n",
        "\n",
        "  if situation:\n",
        "    num_inference_steps = i_inference_steps\n",
        "    guidance_scale = i_guidance_scale\n",
        "\n",
        "    if i_initial_seed is not None:\n",
        "      generator = torch.Generator(\"cuda\").manual_seed(i_initial_seed)\n",
        "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]\n",
        "    else:\n",
        "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]\n",
        "  else:\n",
        "    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
        "\n",
        "  del StableDiffusionPipeline\n",
        "  return image\n",
        "\n",
        "def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):\n",
        "  from tqdm import tqdm\n",
        "  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "  import torchvision\n",
        "  import BigGAN_utils.utils as utils\n",
        "  import clip\n",
        "  import torch.nn.functional as F\n",
        "  from DiffAugment_pytorch import DiffAugment\n",
        "  import numpy as np\n",
        "  from fusedream_utils import FuseDreamBaseGenerator, get_G\n",
        "  import sys\n",
        "\n",
        "  sentence = user_prompt\n",
        "\n",
        "  if use_seed:\n",
        "    utils.seed_rng(SEED)\n",
        "\n",
        "  sys.argv = ['']\n",
        "\n",
        "  G, config = get_G(512)\n",
        "  generator = FuseDreamBaseGenerator(G, config, 10)\n",
        "  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)\n",
        "\n",
        "  z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
        "  y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
        "  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)\n",
        "  ### Set latent_noise = True yields slightly higher AugCLIP score, but slightly lower image quality.\n",
        "\n",
        "  image = img\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  image = pil_images[0]\n",
        "\n",
        "  del tqdm\n",
        "  del torchvision\n",
        "  del utils\n",
        "  del clip\n",
        "  del F\n",
        "  del DiffAugment\n",
        "  del FuseDreamBaseGenerator\n",
        "  del get_G\n",
        "  return image\n",
        "\n",
        "#####################################\n",
        "\n",
        "def main():\n",
        "  #icon = Image.open('/content/icon.png')                                       # Remove comment\n",
        "  st.set_page_config(layout=\"wide\")                                             # Add page_icon = icon within the (), before layout\n",
        "\n",
        "  #background_image = Image.open('/content/background.jpg')                     # Remove comment\n",
        "\n",
        "\n",
        "  ##################################### For red background (Does not work):\n",
        "  page_bg_img = '''\n",
        "  <style>\n",
        "  body {\n",
        "  background-image: background_image\n",
        "  background-size: cover;\n",
        "  }\n",
        "  </style>\n",
        "  '''\n",
        "  st.markdown(page_bg_img, unsafe_allow_html=True)\n",
        "  #####################################\n",
        "\n",
        "  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))\n",
        "\n",
        "  #image = Image.open('/content/akbank.jpg')                                    # Remove comment\n",
        "  #image2 = Image.open('/content/sabancı.png')                                  # Remove comment\n",
        "\n",
        "  with col1:\n",
        "    #st.image(image, caption='', width = 500, use_column_width=True)            # Remove comment\n",
        "    with st.sidebar:\n",
        "      st.title(':red[MODELS]')\n",
        "      st.header('General model information', divider='red')\n",
        "\n",
        "      with st.expander(\"Stable Diffusion v1-4\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"Stable Diffusion 2-1\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"FuseDream (CLIP+BigGAN)\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"Non-specified Flow based model\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      st.header('Settings to tweak the models', divider='red')\n",
        "\n",
        "      situation = st.toggle('Use custom parameters for models')\n",
        "      if situation:\n",
        "        with st.expander('Diffusion model parameters:'):\n",
        "\n",
        "          i_inference_steps = st.number_input('Number of inference steps', step = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)\n",
        "          else:\n",
        "            i_initial_seed = None\n",
        "\n",
        "          i_guidance_scale = st.slider('Guidance scale', 1.0, 50.0, 7.5, step = 0.5)\n",
        "\n",
        "        with st.expander('Dreambooth parameters:'):\n",
        "\n",
        "          id_inference_steps = st.number_input('Number of inference steps', step =1, key = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', key = 2, help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            id_initial_seed = st.number_input('Initial seed', value = 0, step = 1, key = 3)\n",
        "          else:\n",
        "            id_initial_seed = None\n",
        "\n",
        "          id_learning_rate = st.slider('Learning rate', 1, 6, step = 1)\n",
        "          id_learning_rate = id_learning_rate * 1e-6\n",
        "\n",
        "          id_training_steps = st.number_input('Training steps', step =1)\n",
        "\n",
        "          id_save_interval = st.number_input('Save interval', step =1, help = 'Should be lower than the number of training steps!')\n",
        "\n",
        "          id_train_batch_size = st.number_input('Training batch size', step =1, help = 'Should be at least 1 and at most the number of images you have uploaded!')\n",
        "\n",
        "          id_num_class_images = st.number_input('Number of classification images', step =1)\n",
        "\n",
        "        with st.expander('FuseDream (CLIP+BigGAN):'):\n",
        "\n",
        "          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)\n",
        "\n",
        "          OPT_ITERS = st.number_input('Number of iterations', step = 1)\n",
        "\n",
        "          NUM_BASIS = st.number_input('Number of basis images', step = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
        "          else:\n",
        "            SEED = None\n",
        "\n",
        "        with st.expander('Non-specified Flow based model parameters:'):\n",
        "          st.write(\"PLaceholder.\")\n",
        "\n",
        "  with col2:\n",
        "    st.title(\"Akbank Avatar Generator\")\n",
        "\n",
        "    st.write('Show us what you look like 😊')\n",
        "    uploaded_file = st.file_uploader(\"Upload your photos here...\", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')\n",
        "    st.write('Attention: Fine tuning with external data is currently only available for diffusion models 😅')\n",
        "\n",
        "    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = \"Select your model...\")\n",
        "\n",
        "    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')\n",
        "\n",
        "    user_n_prompt = st.text_input('Negative prompt (Optional)', '')\n",
        "    st.write('Attention: Negative prompts are currently only available for diffusion models')\n",
        "\n",
        "    generation_start = st.button('Start generating')\n",
        "    if generation_start and user_prompt == '':\n",
        "      st.error('Please enter a prompt', icon=\"🚨\")\n",
        "    elif generation_start and user_prompt != '':\n",
        "      process = st.empty()\n",
        "      process.text('Generating...')\n",
        "\n",
        "      if option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1':\n",
        "        if not situation:\n",
        "          i_inference_steps = 0\n",
        "          i_guidance_scale = 0\n",
        "          i_initial_seed = 0\n",
        "\n",
        "        generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)\n",
        "\n",
        "        process.empty()            # Until the other models are implemented keep these 3 rows here. After, place them just before with col3, lined up to the beggining of the above elif.\n",
        "        st.write('')\n",
        "\n",
        "        st.image(generated_image, caption= f'Generated image using {option}.')\n",
        "\n",
        "      elif option == 'FuseDream (CLIP+BigGAN)':\n",
        "        if not situation:\n",
        "          INIT_ITERS =  1000\n",
        "          OPT_ITERS = 1000\n",
        "          NUM_BASIS = 10\n",
        "          SEED = None\n",
        "          use_seed = False\n",
        "\n",
        "        generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)\n",
        "\n",
        "        process.empty()            # Until the other models are implemented keep these 3 rows here. After, place them just before with col3, lined up to the beggining of the above elif.\n",
        "        st.write('')\n",
        "\n",
        "        st.image(generated_image, caption= f'Generated image using {option}.')\n",
        "\n",
        "      elif option == 'Non-specified Flow based model':\n",
        "        st.write('This model is not implemented yet', key = 51)\n",
        "\n",
        "  with col3:\n",
        "    for i in range(60):\n",
        "      st.write('')\n",
        "    #st.image(image2, caption='', width = 150, use_column_width=False, output_format=\"PNG\")              # Remove comment\n",
        "\n",
        "#####################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWkZM_DkW4we"
      },
      "outputs": [],
      "source": [
        "# V2: (Shelved)\n",
        "\n",
        "%%writefile akbankavatargenerator.py\n",
        "#####################################\n",
        "import streamlit as st\n",
        "import base64\n",
        "import torch\n",
        "from PIL import Image\n",
        "#####################################\n",
        "# Functions:\n",
        "\n",
        "# Function for calling any of the two stable diffusion models, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
        "def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "\n",
        "  if option == \"Stable Diffusion v1-4\":        # Uses a PNDM scheduler\n",
        "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "  elif option == \"Stable Diffusion 2-1\":\n",
        "    model_id = \"stabilityai/stable-diffusion-2-1\" # Uses a DDIM scheduler\n",
        "\n",
        "  device = \"cuda\"\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "  pipe = pipe.to(device)\n",
        "\n",
        "  prompt = user_prompt\n",
        "  negative_prompt = user_n_prompt\n",
        "\n",
        "  if situation:     # If custom parameters are chosen, parameters are given the user input values. If not, the pipeline is called without change.\n",
        "    num_inference_steps = i_inference_steps\n",
        "    guidance_scale = i_guidance_scale\n",
        "\n",
        "    if i_initial_seed is not None:        # In case of wanting to use custom seed, use the user input value. If not the pipeline is called without seed input.\n",
        "      generator = torch.Generator(\"cuda\").manual_seed(i_initial_seed)\n",
        "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]\n",
        "    else:\n",
        "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]\n",
        "  else:\n",
        "    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
        "\n",
        "  del StableDiffusionPipeline\n",
        "  return image\n",
        "\n",
        "# Function for calling FuseDream model, imports and deletes required modules, parameters are self explanatory returns the image:\n",
        "def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):\n",
        "  from tqdm import tqdm\n",
        "  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "  import torchvision\n",
        "  import BigGAN_utils.utils as utils\n",
        "  import clip\n",
        "  import torch.nn.functional as F\n",
        "  from DiffAugment_pytorch import DiffAugment\n",
        "  import numpy as np\n",
        "  from fusedream_utils import FuseDreamBaseGenerator, get_G\n",
        "  import sys\n",
        "\n",
        "  sentence = user_prompt\n",
        "\n",
        "  if use_seed:           # If the function is called using default parameter values SEED and use_seed are given the value None. If the utils.seed_rng(SEED) is called with None it gives an error, therefore the if statment is used.\n",
        "    utils.seed_rng(SEED)\n",
        "\n",
        "  sys.argv = ['']\n",
        "\n",
        "  G, config = get_G(512)\n",
        "  generator = FuseDreamBaseGenerator(G, config, 10)\n",
        "  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)\n",
        "\n",
        "  z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
        "  y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
        "  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)\n",
        "\n",
        "  image = img\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  image = pil_images[0]\n",
        "\n",
        "  del tqdm\n",
        "  del torchvision\n",
        "  del utils\n",
        "  del clip\n",
        "  del F\n",
        "  del DiffAugment\n",
        "  del FuseDreamBaseGenerator\n",
        "  del get_G\n",
        "  return image\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "def main():\n",
        "  icon = Image.open('/content/icon.png')\n",
        "  st.set_page_config(page_icon = icon, layout=\"wide\")\n",
        "\n",
        "  #background_image = Image.open('/content/background.jpg')\n",
        "\n",
        "  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))\n",
        "\n",
        "  image = Image.open('/content/akbank.jpg')\n",
        "  image2 = Image.open('/content/sabancı.png')\n",
        "\n",
        "  with col1:\n",
        "    st.image(image, caption='', width = 500, use_column_width=True)\n",
        "    with st.sidebar:\n",
        "      st.title(':red[MODELS]')\n",
        "      st.header('General model information', divider='red')\n",
        "\n",
        "      with st.expander(\"Stable Diffusion v1-4\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"Stable Diffusion 2-1\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"FuseDream (CLIP+BigGAN)\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      with st.expander(\"Non-specified Flow based model\"):\n",
        "        st.write(\"PLaceholder.\")\n",
        "\n",
        "      st.header('Settings to tweak the models', divider='red')\n",
        "      # situation = True --> Want to use custom parameters, situation = False --> Want to use default parameters\n",
        "      situation = st.toggle('Use custom parameters for models')\n",
        "      if situation:\n",
        "        with st.expander('Diffusion model parameters:'):  # Have 3 parameters\n",
        "\n",
        "          i_inference_steps = st.number_input('Number of inference steps', step = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)\n",
        "          else:\n",
        "            i_initial_seed = None\n",
        "\n",
        "          i_guidance_scale = st.slider('Guidance scale', 1.0, 50.0, 7.5, step = 0.5)\n",
        "\n",
        "        with st.expander('FuseDream (CLIP+BigGAN) parameters:'): # Has 5 parameters (including the boolean use_seed)\n",
        "\n",
        "          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)\n",
        "\n",
        "          OPT_ITERS = st.number_input('Number of iterations', step = 1)\n",
        "\n",
        "          NUM_BASIS = st.number_input('Number of basis images', step = 1)\n",
        "\n",
        "          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "          if use_seed:\n",
        "            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
        "          else:\n",
        "            SEED = None\n",
        "\n",
        "        with st.expander('Non-specified Flow based model parameters:'): # Not yet implemented, has 0 parameters\n",
        "          st.write(\"PLaceholder.\")\n",
        "\n",
        "      st.header(' ', divider='red')\n",
        "\n",
        "      with st.expander('Dreambooth parameters:'): # Has 9 parameters, Dreambooth is separated from the situation block since it should not be called with default parameters as the parameters need to be entered according to the external data and its use. (TLDR Attune parameters per each use for good results.)\n",
        "\n",
        "        u_token = st.text_input('Unique token', '', placeholder='Enter a unique token.', help = 'example: name_surname or zwx')\n",
        "\n",
        "        u_class = st.text_input('Class of reg images', '', placeholder='Enter a regularization image set class.', help = 'if restricted amount of available image sets, example: list of the available class names; else example: dog or person')\n",
        "\n",
        "        id_inference_steps = st.number_input('Number of inference steps', step =1, key = 1)\n",
        "\n",
        "        use_seed = st.toggle('Enter a seed value', key = 2, help = 'If seed value not entered, each generated image will be given a random one.')\n",
        "        if use_seed:\n",
        "          id_initial_seed = st.number_input('Initial seed', value = 0, step = 1, key = 3)\n",
        "        else:\n",
        "          id_initial_seed = None\n",
        "\n",
        "        id_learning_rate = st.slider('Learning rate', 1, 6, step = 1)\n",
        "        id_learning_rate = id_learning_rate * 1e-6\n",
        "\n",
        "        id_training_steps = st.number_input('Training steps', step =1)\n",
        "\n",
        "        id_save_interval = st.number_input('Save interval', step =1, help = 'Should be lower than the number of training steps!')\n",
        "\n",
        "        id_train_batch_size = st.number_input('Training batch size', step =1, help = 'Should be at least 1 and at most the number of images you have uploaded!')\n",
        "\n",
        "        id_num_class_images = st.number_input('Number of classification images', step =1)\n",
        "\n",
        "  with col2:\n",
        "    st.title(\"Akbank Avatar Generator\")\n",
        "    use_db = st.toggle('Use Dreambooth')  # If you want to use Dreambooth, reveals the following untill the line:\n",
        "    if use_db:\n",
        "      st.write('Show us what you look like 😊')\n",
        "      uploaded_file = st.file_uploader(\"Upload your photos here...\", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')\n",
        "      information = st.checkbox('See additional information for fine tuned image generation')\n",
        "      if information:               # Additional infromation toggle\n",
        "        st.write('PLaceholder')\n",
        "      st.write('Attention: Fine tuning with external data is currently only available for diffusion models 😅')\n",
        "################################################################################\n",
        "    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = \"Select your model...\")\n",
        "\n",
        "    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')\n",
        "\n",
        "    user_n_prompt = st.text_input('Negative prompt (Optional)', '')\n",
        "    st.write('Attention: Negative prompts are currently only available for diffusion models')\n",
        "\n",
        "    generation_start = st.button('Start generating')\n",
        "    if generation_start and user_prompt == '':          # Ensures a prompt is entered\n",
        "      st.error('Please enter a prompt', icon=\"🚨\")\n",
        "    elif generation_start and user_prompt != '':\n",
        "\n",
        "      select_sd = option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1'\n",
        "\n",
        "      if use_db and not select_sd:                                     # Ensures that a diffusion model is selected in the case of wanting to use Dreambooth\n",
        "        st.error('Please select a diffusion model', icon=\"🚨\")\n",
        "      else:\n",
        "\n",
        "        process = st.empty()            # Creating a Generating...\n",
        "        process.text('Generating...')\n",
        "\n",
        "        if use_db and select_sd:     # Selected model is a Stable Diffusion one with Dreambooth\n",
        "          process.empty()            # Deletes Generating... text\n",
        "          st.write('')\n",
        "\n",
        "          st.write('Error: This model is not implemented yet', key = 51)\n",
        "\n",
        "        elif select_sd:       # Selected model is a Stable Diffusion one\n",
        "          if not situation:\n",
        "            i_inference_steps = 0         # If chosen to use default parameters, gives placeholder values that will be changed to default values within the stablediffusioncall function\n",
        "            i_guidance_scale = 0\n",
        "            i_initial_seed = 0\n",
        "\n",
        "          generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)       # Calls stablediffusioncall function\n",
        "\n",
        "          process.empty()            # Deletes Generating... text\n",
        "          st.write('')\n",
        "\n",
        "          st.image(generated_image, caption= f'Generated image using {option}.')       # Display the generated image with caption\n",
        "\n",
        "        elif option == 'FuseDream (CLIP+BigGAN)':    # Selected model is FuseDream\n",
        "          if not situation:\n",
        "            INIT_ITERS =  1000\n",
        "            OPT_ITERS = 1000                 # If chosen to use default parameters, gives default values\n",
        "            NUM_BASIS = 10\n",
        "            SEED = None\n",
        "            use_seed = False\n",
        "\n",
        "          generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)            # Calls fusedreamcall function\n",
        "\n",
        "          process.empty()           # Deletes Generating... text\n",
        "          st.write('')\n",
        "\n",
        "          st.image(generated_image, caption= f'Generated image using {option}.')    # Display the generated image with caption\n",
        "\n",
        "        elif option == 'Non-specified Flow based model':      # Selected model is a Non-specified Flow based model\n",
        "          process.empty()                  # Deletes Generating... text\n",
        "          st.write('')\n",
        "\n",
        "          st.write('Error: This model is not implemented yet', key = 51)\n",
        "\n",
        "  with col3:\n",
        "    st.image(image2, caption='', width = 150, use_column_width=True, output_format=\"PNG\")\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0OUZEbgG180"
      },
      "source": [
        "SANDBOX:\n",
        "---------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2qnGj3Olj24"
      },
      "outputs": [],
      "source": [
        "# Dreambooth requirements:\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton                                                 # use either this or the one at the bottom\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKJMf-kSlpZl"
      },
      "outputs": [],
      "source": [
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgtQGTYnG_Aj"
      },
      "outputs": [],
      "source": [
        "# Settings (including model selection):\n",
        "\n",
        "u_token=\"yigitya\"\n",
        "\n",
        "u_class=\"person\"\n",
        "\n",
        "plant=1337\n",
        "\n",
        "t_b_size=2\n",
        "\n",
        "l_r=1e-6\n",
        "\n",
        "n_c_images=100\n",
        "\n",
        "s_b_size=4\n",
        "\n",
        "m_t_steps=640\n",
        "\n",
        "s_prompt=u_token+\" \"+u_class\n",
        "\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/stable_diffusion_weights/\" + u_token\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f31noCw2JIgk"
      },
      "outputs": [],
      "source": [
        "# Choosing the instance and class prompts:\n",
        "\n",
        "# You can also add multiple concepts here, try tweaking `--max_train_steps` accordingly.\n",
        "# `class_data_dir` contains the regularization images\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f\"photo of {u_token} man\",\n",
        "        \"class_prompt\":         \"photo of a man\",\n",
        "        \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
        "        \"class_data_dir\":       \"/content/data/man\"\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      f\"photo of {u_token} woman\",\n",
        "        \"class_prompt\":         \"photo of a woman\",\n",
        "        \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
        "        \"class_data_dir\":       \"/content/data/woman\"\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      f\"photo of {u_token} person\",\n",
        "        \"class_prompt\":         \"photo of a person\",\n",
        "        \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
        "        \"class_data_dir\":       \"/content/data/person\"\n",
        "    }\n",
        "]\n",
        "\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "keep_cl = []\n",
        "for c in concepts_list:\n",
        "  a_class = c['instance_prompt'].split()\n",
        "  if a_class[-1] == u_class:\n",
        "    keep_cl.append(c)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(keep_cl, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mp0hHitPKqf"
      },
      "outputs": [],
      "source": [
        "# Upload your images by running this cell:\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "  a_class = c['instance_prompt'].split()\n",
        "  if a_class[-1] == u_class:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuNAt6TNUYBs"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning the SD model:\n",
        "\n",
        "# Tweak the parameters for desired image quality --> 1200 training steps = 40 newly introdued images for 30 epochs.\n",
        "!python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=$plant \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=$t_b_size \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=$l_r \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=$n_c_images \\\n",
        "  --sample_batch_size=$s_b_size \\\n",
        "  --max_train_steps=$m_t_steps \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"$s_prompt\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlEiJBZmXl0G"
      },
      "outputs": [],
      "source": [
        "# Specify the weights directory to use (leave blank for latest):\n",
        "\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import os\n",
        "WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRHh1T5CYThG"
      },
      "outputs": [],
      "source": [
        "# Inference:\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x-IZ6yYYeCQ"
      },
      "outputs": [],
      "source": [
        "# Can set a random seed here for reproducibility:\n",
        "\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eApBIYYqhjxs"
      },
      "outputs": [],
      "source": [
        "# Run for generating images.\n",
        "\n",
        "prompt = \"A version of yigitya person as an cosmonaut with the helmet and suit in outher space.\"\n",
        "negative_prompt = \"weird eyes, blurry eyes, fat \"\n",
        "num_samples = 1\n",
        "guidance_scale = 9\n",
        "num_inference_steps = 100\n",
        "height = 512\n",
        "width = 512\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_cPOrXbFkvsd",
        "_0OUZEbgG180"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}